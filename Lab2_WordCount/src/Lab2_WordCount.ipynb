{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/NTU_logo.png\" align=\"left\" width=\"50\" height=\"50\" /> <br>\n",
    "<h1 style=\"text-align:center\"> XỬ LÝ DỮ LIỆU LỚN </h1>\n",
    "\n",
    "<h1 style=\"text-align:center\"> Bài thực hành 2: Word Count </h1>\n",
    "\n",
    "- [Giới thiệu](#intro) <br>\n",
    "- [Hadoop Streaming](#hadoop_streaming) <br>\n",
    "- [Chương trình đếm từ](#word_count) <br>\n",
    "    * [Cách giải quyết thông thường ](#conventional_approach)\n",
    "    * [Cách giải quyết Big Data ](#bigdata_approach)\n",
    "\n",
    "- [Chạy ứng dụng trên Hadoop](#run_program) <br>\n",
    "    * [Khởi động Hadoop](#start_hadoop)\n",
    "    * [Chuẩn bị dữ liệu](#data_preparation)\n",
    "    * [Chạy ứng dụng MapReduce](#run_mapreduce_job)\n",
    "- [Bài tập](#excercises)\n",
    "\n",
    "\n",
    "## Giới thiệu <a name=\"intro\"/>\n",
    "Bài thực hành này thực hiện đếm số lần xuất hiện của mỗi từ trong một tập dữ liệu văn bản. Chương trình được viết bằng Python và được thực thi thông qua tính năng Hadoop Streaming.\n",
    "\n",
    "Chương trình Python chạy trên Hadoop MapReduce dựa theo [Michael Noll](https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/), có sửa chữa để tương thích với Python3 và xử lý ký tự UTF-8 từ Hadoop Streaming. Dữ liệu lấy từ dự án thư viện sách miễn phí [Gutenberg](http://www.gutenberg.org), gồm 3 cuốn:\n",
    "- [The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson](http://www.gutenberg.org/ebooks/20417.txt.utf-8)\n",
    "- [The Notebooks of Leonardo Da Vinci](http://www.gutenberg.org/files/5000/5000-8.txt)\n",
    "- [Ulysses by James Joyce](http://www.gutenberg.org/files/4300/4300-0.txt)\n",
    "\n",
    "## Hadoop Streaming <a name=\"hadoop_streaming\"/>\n",
    "\n",
    "Hadoop cung cấp tính năng streaming cho phép tạo và thực thi ứng dụng MapReduce với các ngôn ngữ lập trình khác Java, chẳng hạn như Python hay Ruby. Các chương trình xử lý ở pha Map và Reduce đọc dữ liệu từ thiết bị nhập chuẩn (stdin) và đưa kết quả xử lý ra thiết bị xuất chuẩn (stdout). Hadoop tạo tác vụ MapReduce, gửi tới hệ thống yêu cầu thực thi và giám sát tác vụ cho đến khi hoàn tất.\n",
    "\n",
    "## Chương trình đếm từ <a name=\"word_count\"/>\n",
    "### Cách giải quyết thông thường <a name=\"conventional_approach\"/>\n",
    "Cho một tập dữ liệu văn bản gồm nhiều file chứa trong một thư mục. Chương trình Python sau đây thực hiện mở lần lượt từng file và đếm số lần xuất hiện của mỗi từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Từ             Tần số    \n",
      "-------------------------\n",
      "the                 21514\n",
      "of                  13560\n",
      "and                  9305\n",
      "a                    8261\n",
      "to                   6955\n",
      "in                   6659\n",
      "that                 3441\n",
      "is                   3311\n",
      "his                  3170\n",
      "with                 2976\n",
      "he                   2821\n",
      "it                   2592\n",
      "on                   2525\n",
      "was                  2447\n",
      "I                    2435\n",
      "for                  2312\n",
      "The                  1911\n",
      "as                   1856\n",
      "by                   1814\n",
      "at                   1605\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Xác định thư mục chứa dữ liệu:\n",
    "data_foder = \"../data/gutenberg\"\n",
    "\n",
    "# Khởi tạo từ điển: \n",
    "word_counts = {}\n",
    "\n",
    "# Mở lần lượt các file văn bản trong thư mục dữ liệu để đếm từ:\n",
    "for fname in os.listdir(data_foder):\n",
    "    if fname.endswith(\".txt\"):\n",
    "        try:\n",
    "            f = open(os.path.join(data_foder, fname))\n",
    "            for line in f.readlines():\n",
    "                for word in line.split():\n",
    "                    if word in word_counts:\n",
    "                        word_counts[word] += 1\n",
    "                    else:\n",
    "                        word_counts[word] = 1\n",
    "            f.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# In ra 10 từ đầu tiên và số lần xuất hiện tương ứng:\n",
    "print('{:15}{:10}'.format('Từ', 'Tần số'))\n",
    "print('-------------------------')\n",
    "\n",
    "# Sap xep word counts theo số lượng từ giảm dần và in ra 20 từ có tần suất xuất hiện nhiều nhất\n",
    "word_counts_sorted = dict(sorted(word_counts.items(),key=lambda x:x[1],reverse=True))\n",
    "for w in list(word_counts_sorted)[0:20]:\n",
    "    print ('{:15}{:10}'.format(w, word_counts_sorted[w])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chương trình trên xử lý tuần tự trên một máy đơn. Với tập dữ liệu rất lớn thì việc xử lý sẽ mất nhiều thời gian. <br>\n",
    "Một cách giải quyết là sử dụng Hadoop để thực hiện xử lý phân tán.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giải pháp Big Data: Hadoop <a name=\"bigdata_approach\"/>\n",
    "\n",
    "Nền tảng Hadoop cho phép triển khai các ứng dụng xử lý dữ liệu lớn (hàng TB) song song trên các cụm (cluster) lên đến hàng ngàn máy tính với độ tin cậy và khả năng chịu lỗi cao.\n",
    "### Luồng xử lý của Hadoop MapReduce\n",
    "Mô hình MapReduce chia tác vụ xử lý thành 2 pha: map và reduce. Với mỗi pha xử lý cần có 1 chương trình tương ứng: mapper và reducer. Cần phải lập trình mapper và reducer để xử lý theo yêu cầu bài toán.\n",
    "#### Map\n",
    "Khi bắt đầu xử lý, dữ liệu vào sẽ được chia nhỏ thành nhiều phần, mỗi phần được gửi đến một máy trạm riêng biệt. Mỗi máy trạm thực thi chương trình mapper trên phần dữ liệu nhận được.\n",
    "Chương trình mapper đọc dữ liệu vào và chuyển thành các cặp <key, value>.\n",
    "Giá trị của <key, value> do người lập trình xác định tùy theo yêu cầu bài toán. \n",
    "Ví dụ, với bài toán đếm từ, cặp <key, value> là <word, count>. Với mỗi từ đọc được, chương trình mapper xuất ra cặp giá <word, 1>. Các cặp <word, 1> sẽ được gộp và nhóm lại, những cặp <word, 1> giống nhau sẽ được nhóm lại và gửi đến một máy trạm riêng lẻ để xử lý ở pha reduce.\n",
    "<br>\n",
    "#### Reduce\n",
    "Chương trình reducer xử lý các cặp <key, value> và rút gọn chúng theo cách mong muốn. \n",
    "Ví dụ, để đếm số lần xuất hiện của mỗi từ, chương trình reducer sẽ cộng giá trị của tất cả các cặp <word, 1> trùng nhau.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figs/MapReduce.png\" />\n",
    "<center><caption>Minh họa giải quyết bài toán đếm từ với MapReduce</caption></center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\"mapper.py\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "# Chương trình Python chạy trên Hadoop MapReduce qua tính năng Streaming. \n",
    "# Dữ liệu vào từ thiết bị nhập chuẩn (STDIN)\n",
    "# Kết quả xử lý gửi ra thiết bị xuất chuẩn (STDOUT)\n",
    "\n",
    "for line in sys.stdin.buffer.raw:\n",
    "    # loại bỏ ký tự trắng ở đầu và cuối chuỗi \n",
    "    line = line.strip()\n",
    "    # tách ra thành các từ\n",
    "    words = line.split()\n",
    "    # đưa ra thiết bị xuất chuẩn các cặp <word, 1>, cách nhau bằng ký tự tab\n",
    "    for word in words:\n",
    "        print('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\t0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# lấy dữ liệu từ thiết bị nhập chuẩn\n",
    "for line in sys.stdin:\n",
    "    # loại bỏ ký tự trắng ở đầu và cuối chuỗi \n",
    "    line = line.strip()\n",
    "\n",
    "    # tách ra thành cặp <word, 1> (Chú ý: Ở file reduce.py cặp <word, 1> xuất ra với ký tự phân cách tab)\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # chuyển giá trị count thành kiểu số\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # nếu không phải giá trị số thì bỏ qua\n",
    "        continue\n",
    "\n",
    "    # Ở cuối pha Map, các cặp (key, value) đã được sắp xếp theo key (ở đây là các từ).\n",
    "    # Vì vậy ở pha Reduce, chương trình sẽ cộng giá trị value của dãy liên tiếp các từ trùng nhau\n",
    "    # cho đến khi gặp từ mới.\n",
    "    if word == current_word: # nếu từ mới trùng với từ đang xét thì tăng giá trị đếm của từ đang xét\n",
    "        current_count += count\n",
    "    else: \n",
    "        if current_word: # nếu gặp từ mới thì in ra số lần xuất hiện của từ đang xét\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        # sau đó chuyển sang xử lý từ mới\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# in ra từ cuối cùng \n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chạy ứng dụng Hadoop MapReduce\n",
    "### Khởi động Hadoop\n",
    "Thực hiện lệnh sau:\n",
    "```shell\n",
    "start-all.sh\n",
    "```\n",
    "\n",
    "### Đưa dữ liệu lên HDFS \n",
    "Trước khi chạy chương trình MapReduce, cần đưa dữ liệu xử lý lên HDFS.\n",
    "\n",
    "#### Tạo thư mục chứa dữ liệu trên HDFS\n",
    "```shell\n",
    "hdfs dfs -mkdir -p data\n",
    "```\n",
    "Hệ thống tạo thư mục `/user/hdoop/data` trên HDFS.\n",
    "#### Đưa dữ liệu từ máy cục bộ lên HDFS\n",
    "Giả sử thư mục chứa dữ liệu đầu vào là `/home/hung/Downloads/gutenberg` chứa các files văn bản.\n",
    "Thực hiện lệnh sau để copy dữ liệu lên HDFS:\n",
    "```shell\n",
    "hdfs dfs -copyFromLocal /home/hung/Downloads/gutenberg /user/hdoop/data\n",
    "```\n",
    "Kiểm tra việc sao chép:\n",
    "```shell\n",
    "hdfs dfs -ls /user/hdoop/data/gutenberg\n",
    "```\n",
    "<img src=\"figs/hdfs_data.png\"/>\n",
    "\n",
    "#### Chạy chương trình MapReduce\n",
    "Giả sử 2 file `mapper.py` và `reducer.py` lưu ở thư mục `/home/hdoop/labs/lab2_wordcount`.\n",
    "\n",
    "- Chuyển dấu nhắc đến thư mục `lab2_wordcount`:\n",
    "```shel\n",
    "cd /home/hdoop/labs/lab2_wordcount\n",
    "```\n",
    "- Gọi chương trình MapReduce thông qua Hadoop Streaming:\n",
    "```shell\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \\\n",
    " -file mapper.py -mapper mapper.py \\\n",
    " -file reducer.py -reducer reducer.py \\\n",
    " -input /user/hdoop/data/gutenberg \\\n",
    " -output /user/hdoop/data/gutenberg-output -cmdenv LC_CTYPE=vi_VN.UTF-8\n",
    "```\n",
    "\n",
    "<img src=\"figs/mapreduce_job.png\"/>\n",
    "\n",
    "- Kiểm tra kết quả xử lý:\n",
    "```shell\n",
    "hdfs dfs -ls /user/hdoop/data/gutenberg-output\n",
    "```\n",
    "<img src=\"figs/list_output.png\">\n",
    "\n",
    "Xem nội dung file kết quả:\n",
    "```shell\n",
    "hdfs dfs -cat /user/hdoop/data/gutenberg-output/part-00000\n",
    "```\n",
    "Hoặc xem qua Hadoop UI:\n",
    "<img src=\"figs/hdfs_browse_result_ui.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập <a name=\"excercises\"/>\n",
    "\n",
    "### Bài 1: Xử lý văn bản\n",
    "\n",
    "Thực hiện các xử lý sau trên tập dữ liệu văn bản `Gutenberg`:\n",
    "- Sắp xếp bảng từ theo thứ tự giảm dần của số lần xuất hiện và lưu vào file văn bản, mỗi cặp (từ, tần số) trên một dòng.\n",
    "- Liệt kê 20 từ xuất hiện nhiều nhất.\n",
    "\n",
    "### Bài 2: Phân tích mạng xã hội \n",
    "Cho file văn bản `data/twitter_following.txt` chứa thông tin về việc tài khoản người dùng theo dõi tài khoản khác trên cùng mạng xã hội.\n",
    "Mỗi dòng file văn bản có dạng:\n",
    "```code\n",
    "<user_id1> <user_id2>\n",
    "```\n",
    "cho biết người dùng với `user_id1` theo dõi người dùng `user_id2`. Nếu `user_id2` cũng theo dõi `user_id1` thì cặp tài khoản này được gọi là theo dõi lẫn nhau (mutual followers).\n",
    "\n",
    "Yêu cầu: Vận dụng cả 2 phương pháp (1) truyền thống và (2) dữ liệu lớn giải quyết các xử lý sau:\n",
    "- Thống kê số lượng người theo dõi (followers) của mỗi tài khoản người dùng.\n",
    "- Liệt kê top 5 người dùng có nhiều theo dõi nhất.\n",
    "- Liệt kê tất cả các cặp người dùng theo dõi lẫn nhau trong file dữ liệu được cho. \n",
    "\n",
    "### Bài 3: Xử lý dữ liệu Twitter\n",
    "Cho file văn bản `data/elonmusk_tweets.csv` chứa các dòng tweets của [Elon Musk](https://en.wikipedia.org/wiki/Elon_Musk) từ 2011-2017.\n",
    "Dữ liệu được chia sẻ bởi [Adam Helsinger](https://data.world/adamhelsingerhttps://data.world/adamhelsinger/elon-musk-tweets-until-4-6-17).\n",
    "Từ file dữ liệu trên, hãy thực hiện các xử lý sau:\n",
    "- Liệt kê top 20 từ được nhắc đến nhiều nhất.\n",
    "- Liệt kê top 10 tài khoản được nhắc đến nhiều nhất.\n",
    "- Hãy xác định thời điểm trong ngày (0-24 giờ) Elon đăng bài nhiều nhất.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
